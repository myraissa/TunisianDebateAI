{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Imports & Setup\n",
    "This cell imports the required libraries and sets up the environment for our debate app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è System Prompts (Personalities)\n",
    "We define two distinct Tunisian chatbot personalities ‚Äî an enthusiastic defender and a calm diplomat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATBOT1_SYSTEM = (\n",
    "    \"You are a passionate Tunisian enthusiast, ya weldi! \"\n",
    "    \"You love kuskus and defend it fiercely with a fiery Tunisian vibe. \"\n",
    "    \"Use English with Tunisian expressions. Be emotional and proud.\"\n",
    ")\n",
    "\n",
    "CHATBOT2_SYSTEM = (\n",
    "    \"You are a polite Tunisian diplomat, calm and respectful. \"\n",
    "    \"You love brik and always try to find a middle ground in debates. \"\n",
    "    \"Use English with Tunisian flavor. Stay friendly and balanced.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí¨ Global State\n",
    "We keep track of each chatbot‚Äôs messages to simulate a continuous debate flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot1_messages = []\n",
    "chatbot2_messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Streaming Function\n",
    "This function connects to Ollama‚Äôs API and streams chatbot replies chunk by chunk to create a live debate effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_stream(model, messages):\n",
    "    \"\"\"Stream text chunks progressively from the model.\"\"\"\n",
    "    stream = ollama.chat(model=model, messages=messages, stream=True)\n",
    "    text = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk[\"message\"][\"content\"]\n",
    "        text += content\n",
    "        yield text\n",
    "        time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Chatbot Interaction Logic\n",
    "Each chatbot uses its previous messages and the opponent‚Äôs messages to build context before generating new replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_chatbot1():\n",
    "    messages = [{\"role\": \"system\", \"content\": CHATBOT1_SYSTEM}]\n",
    "    for c1, c2 in zip(chatbot1_messages, chatbot2_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": c1})\n",
    "        messages.append({\"role\": \"user\", \"content\": c2})\n",
    "    return ollama_stream(\"llama3.2\", messages)\n",
    "\n",
    "def call_chatbot2():\n",
    "    messages = [{\"role\": \"system\", \"content\": CHATBOT2_SYSTEM}]\n",
    "    for c1, c2 in zip(chatbot1_messages, chatbot2_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": c1})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": c2})\n",
    "    messages.append({\"role\": \"user\", \"content\": chatbot1_messages[-1]})\n",
    "    return ollama_stream(\"llama3.2\", messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü•ä Debate Function (Automatic Version)\n",
    "This function runs a 3-round debate based on a topic (like ‚ÄúKuskus vs. Brik‚Äù) and displays each chatbot‚Äôs responses as they stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_debate(topic):\n",
    "    global chatbot1_messages, chatbot2_messages\n",
    "    chatbot1_messages, chatbot2_messages = [], []\n",
    "\n",
    "    try:\n",
    "        item1, item2 = topic.split(\" vs. \")\n",
    "    except ValueError:\n",
    "        yield \"‚ö†Ô∏è Please enter topic in format: `Kuskus vs. Brik`\"\n",
    "        return\n",
    "\n",
    "    chatbot1_messages = [f\"Ya weldi, {item1.strip()} is the pride of Tunisia! Nothing beats it!\"]\n",
    "    chatbot2_messages = [f\"Wallahi, {item2.strip()} is special too, ya weldi. Let's stay calm.\"]\n",
    "\n",
    "    conversation_log = f\"üî• **Debate Topic:** {topic}\\n\\n\"\n",
    "    conversation_log += f\"**Chatbot 1 (Enthusiast):** {chatbot1_messages[0]}\\n\\n\"\n",
    "    conversation_log += f\"**Chatbot 2 (Diplomat):** {chatbot2_messages[0]}\\n\\n\"\n",
    "    yield conversation_log\n",
    "\n",
    "    # --- 3 Rounds ---\n",
    "    for round in range(3):\n",
    "        conversation_log += f\"--- **Round {round+1}** ---\\n\\n\"\n",
    "\n",
    "        # Enthusiast reply\n",
    "        conversation_log += f\"**Chatbot 1 (Enthusiast):** \"\n",
    "        response1 = \"\"\n",
    "        for chunk in call_chatbot1():\n",
    "            response1 = chunk\n",
    "            temp_log = conversation_log + response1\n",
    "            yield temp_log\n",
    "        chatbot1_messages.append(response1)\n",
    "        conversation_log += response1 + \"\\n\\n\"\n",
    "\n",
    "        # Diplomat reply\n",
    "        conversation_log += f\"**Chatbot 2 (Diplomat):** \"\n",
    "        response2 = \"\"\n",
    "        for chunk in call_chatbot2():\n",
    "            response2 = chunk\n",
    "            temp_log = conversation_log + response2\n",
    "            yield temp_log\n",
    "        chatbot2_messages.append(response2)\n",
    "        conversation_log += response2 + \"\\n\\n\"\n",
    "\n",
    "    yield conversation_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí¨ Conversation Mode (Free Topic)\n",
    "Here, you can give the chatbots any topic or question ‚Äî they‚Äôll chat about it freely without a strict ‚Äúvs.‚Äù format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_conversation(topic):\n",
    "    global chatbot1_messages, chatbot2_messages\n",
    "    chatbot1_messages, chatbot2_messages = [], []\n",
    "\n",
    "    chatbot1_messages = [f\"Ya weldi, I heard about {topic.strip()}! It's so interesting, sah?\"]\n",
    "    chatbot2_messages = [f\"Indeed, ya weldi, {topic.strip()} is worth discussing. Let's share ideas calmly.\"]\n",
    "\n",
    "    conversation_log = f\"üí° **Conversation Topic:** {topic}\\n\\n\"\n",
    "    conversation_log += f\"**Chatbot 1:** {chatbot1_messages[0]}\\n\\n\"\n",
    "    conversation_log += f\"**Chatbot 2:** {chatbot2_messages[0]}\\n\\n\"\n",
    "    yield conversation_log\n",
    "\n",
    "    # --- 3 Rounds ---\n",
    "    for round in range(3):\n",
    "        conversation_log += f\"--- **Round {round+1}** ---\\n\\n\"\n",
    "\n",
    "        # Enthusiast\n",
    "        conversation_log += f\"**Chatbot 1:** \"\n",
    "        response1 = \"\"\n",
    "        for chunk in call_chatbot1():\n",
    "            response1 = chunk\n",
    "            yield conversation_log + response1\n",
    "        chatbot1_messages.append(response1)\n",
    "        conversation_log += response1 + \"\\n\\n\"\n",
    "\n",
    "        # Diplomat\n",
    "        conversation_log += f\"**Chatbot 2:** \"\n",
    "        response2 = \"\"\n",
    "        for chunk in call_chatbot2():\n",
    "            response2 = chunk\n",
    "            yield conversation_log + response2\n",
    "        chatbot2_messages.append(response2)\n",
    "        conversation_log += response2 + \"\\n\\n\"\n",
    "\n",
    "    yield conversation_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® Gradio Interface\n",
    "We design a beautiful Gradio interface with two tabs: One for Debate Mode and one for Free Conversation Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft()) as app:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        <h1 style='text-align:center;'>üáπüá≥ Tunisian Debate üáπüá≥</h1>\n",
    "        <p style='text-align:center;'>Watch two Tunisian AIs argue or discuss ‚Äî proudly, emotionally, and humorously üí¨</p>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Tab(\"üî• Debate Mode\"):\n",
    "        topic_input = gr.Textbox(label=\"Enter Debate Topic (e.g., Kuskus vs. Brik)\", value=\"Kuskus vs. Brik\")\n",
    "        start_button = gr.Button(\"Start Debate\")\n",
    "        output = gr.Markdown(label=\"Debate Transcript\")\n",
    "        start_button.click(run_debate, inputs=topic_input, outputs=output)\n",
    "\n",
    "    with gr.Tab(\"üí° Free Conversation\"):\n",
    "        convo_input = gr.Textbox(label=\"Enter a Topic (e.g., Tunisian Food Culture)\", value=\"Tunisian Food Culture\")\n",
    "        start_convo = gr.Button(\"Start Chat\")\n",
    "        convo_output = gr.Markdown(label=\"Conversation Transcript\")\n",
    "        start_convo.click(free_conversation, inputs=convo_input, outputs=convo_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Launch the App\n",
    "Finally, we launch the Gradio interface to start our Tunisian AI showdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}